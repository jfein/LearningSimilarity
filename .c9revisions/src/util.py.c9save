{"ts":1354037096245,"silentsave":true,"restoring":false,"patch":[[{"diffs":[[1,"'''\r\nRequires the following package:\r\n    https://github.com/martinblech/xmltodict\r\n    http://nltk.org/index.html\r\n'''\r\n\r\nimport collections\r\nimport math\r\nimport re\r\nimport string\r\nimport random\r\nimport xmltodict\r\nfrom nltk.stem import WordNetLemmatizer, PorterStemmer\r\nfrom nltk.corpus import wordnet, stopwords\r\nfrom nltk.tokenize import sent_tokenize\r\n\r\n\r\nDATA_PATH = \"../data/data.xml\"\r\nSTOP_WORDS = stopwords.words(\"english\")\r\n\r\nWORD_NET_LEMMATIZER, PORTER_STEMMER = range(2)\r\n\r\n\r\nclass SourceArticles():\r\n\r\n    @property\r\n    def count(self):\r\n        return len(self.articles)\r\n\r\n    def __init__(\r\n            self,\r\n            stdizer=WORD_NET_LEMMATIZER,\r\n            omit_stopwords=False,\r\n            stdize_kws=True,\r\n            stdize_article=False,\r\n            max_phrase_size=None,\r\n        ):\r\n\r\n        self.omit_stopwords = omit_stopwords\r\n        self.stdize_kws = stdize_kws\r\n        self.stdize_article = stdize_article\r\n        self.max_phrase_size = max_phrase_size\r\n        \r\n        if stdizer == WORD_NET_LEMMATIZER:\r\n            self.stdize_word = WordNetLemmatizer().lemmatize\r\n        elif stdizer == PORTER_STEMMER:\r\n            self.stdize_word = PorterStemmer().stem\r\n        \r\n        data = xmltodict.parse(open(DATA_PATH, 'r'))\r\n\r\n        self.articles = []\r\n        self.keywords = {}\r\n\r\n        # Only use article if it has text in the article and is NOT NESTED\r\n        for article in data['database']['table']:\r\n            article_body = article['column'][3].get('#text', 'null').encode('utf-8')\r\n            if article_body.lower() != 'null':\r\n                # Make a simple dict for the article's data\r\n                article_dict = {'article' : article_body}\r\n\r\n                # Add in title info\r\n                t = article['column'][2].get('#text', \"\").encode('utf-8')\r\n                article_dict['title'] = t if t.lower() != \"null\" else \"\"\r\n\r\n                # Add in keyword info\r\n                kws = article['column'][1].get('#text', \"\").encode('utf-8')\r\n                kws = kws if kws.lower() != \"null\" else \"\"\r\n                article_dict['keywords'] = self.format_kws(kws)\r\n\r\n                # Store keyword associations\r\n                for word in article_dict['keywords']:\r\n                    if word not in self.keywords:\r\n                        self.keywords[word] = set()\r\n                    self.keywords[word].add(len(self.articles))\r\n\r\n                # Append article\r\n                self.articles.append(article_dict)\r\n                \r\n    def format_kws(self, kws):\r\n        '''\r\n        Turns a text into a set of standard lemmatized words\r\n        Words returned ONLY if alphabetic, more than 2 chars, and not stop word\r\n        '''\r\n        words = set()\r\n\r\n        # Format kws\r\n        kws = kws.lower().replace(\"|\", \" \").replace(\",\", \" \").replace(\"{\", \"\").replace(\"}\", \"\")\r\n\r\n        # Add each valid word in each keyword phrase\r\n        for word in kws.split(' '):\r\n            word = word.strip()\r\n            # TODO: make this better\r\n            if bool(re.match('[a-z]+$', word, re.IGNORECASE)):\r\n                if (self.stdize_kws):\r\n                    word = self.stdize_word(word)\r\n                if word not in STOP_WORDS and len(word) > 2:\r\n                    words.add(word)\r\n                    \r\n        return words\r\n\r\n    def get_keywords(self, num):\r\n        '''\r\n        Returns the set of article num's keywords\r\n        '''\r\n        return self.articles[num]['keywords']\r\n\r\n    def get_title(self, num):\r\n        '''\r\n        Returns article num's title\r\n        '''\r\n        return self.articles[num]['title']\r\n\r\n    def get_article(self, num):\r\n        '''\r\n        Returns article num's article body\r\n        '''\r\n        return self.articles[num]['article']\r\n\r\n    def spin_article(self, num, article_body=None):\r\n        '''\r\n        Returns a generated spun article. Spun articles are \r\n        represented as a list of spun sentences. A spun \r\n        sentence is simply a string.\r\n        \r\n        So if article is \"I {like|love} the {dog|canine}. He {likes|loves} the {cat|feline}\"\r\n        and n = 2, a possible result would be\r\n        \r\n        [\"I like the canine\", \"He likes the cat\"]\r\n        '''\r\n        article_sentences = self.get_article_sentences(num, article_body)\r\n\r\n        spun_sentences = []\r\n        for sentence in article_sentences:\r\n            spun_sentence = []\r\n            for spin_group in sentence:\r\n                spin_group = list(spin_group)\r\n                phrase = random.choice(spin_group)\r\n                spun_sentence.append(\" \".join(phrase).strip())\r\n            spun_sentences.append(\" \".join(spun_sentence))\r\n\r\n        return spun_sentences\r\n\r\n    def spin_dissimilar_articles(self, num, n=1, article_body=None):\r\n        '''        \r\n        Returns list of n spun articles; articles produced will try to have\r\n        lowest cos similarity; we use the heuristic of choosing a different \r\n        entry from each spin group with the assumption that it will lead to \r\n        some of the lowest cos similarity\r\n        \r\n        So if article is \"I {like|love} the {dog|canine}. He {likes|loves} the {cat|feline}\"\r\n        and n = 2, a possible result would be\r\n        \r\n        [[\"I like the canine\", \"He likes the cat\"], [\"I love the dog\", \"He loves the feline\"]]\r\n        \r\n        Essentially spin_article_sentences but with the n parameter\r\n        '''\r\n        article_sentences = self.get_article_sentences(num, article_body)\r\n        article_sentences_cpy = []\r\n        for sentence in article_sentences:\r\n            sentence_cpy = []\r\n            for spin_group in sentence:\r\n                sentence_cpy.append(list(spin_group))\r\n            article_sentences_cpy.append(sentence_cpy)\r\n\r\n        article_sentences = article_sentences_cpy\r\n\r\n        spun_articles = []\r\n        for _ in xrange(n):\r\n            spun_sentences = []\r\n            for sentence in article_sentences:\r\n                spun_sentence = []\r\n                for spin_group in sentence:\r\n                    phrase = random.choice(spin_group)\r\n                    spun_sentence.append(\" \".join(phrase).strip())\r\n                    if len(spin_group) > 1:\r\n                        spin_group.remove(phrase)\r\n                spun_sentences.append(\" \".join(spun_sentence))\r\n            spun_articles.append(spun_sentences)\r\n\r\n        return spun_articles\r\n\r\n    def get_article_sentences(self, num, article_body=None):\r\n        '''\r\n        Return article_num as a list of sentences. \r\n        Each sentence is represented as a list of spin groups. \r\n        Each spin group is represented as a frozenset of phrases\r\n        Each phrase is represented as a tuple of words.\r\n        \r\n        So returns a set of list of list of sets of lists\r\n        I {like|love} dogs. {They are|one is} great.\r\n        \r\n        [\r\n        [Set([I]), SET([like], [love]), Set([dogs])],\r\n        [Set([they, are], [one, is]), Set([great])]\r\n        ]\r\n        \r\n        The words are standardized based on options passed\r\n        to SourceArticles on construction.\r\n        '''\r\n        if not article_body:\r\n            if 'article_sentences' in self.articles[num]:\r\n                return self.articles[num]['article_sentences']\r\n\r\n            # Use this flag since we can't use 'if not article_body' at the end\r\n            # to determine whether to store results in the cache since\r\n            # we're about to assign something to article_body\r\n            store_in_cache = True\r\n            article_body = self.articles[num]['article']\r\n        else:\r\n            store_in_cache = False\r\n            article_body = article_body.lower()\r\n\r\n        sentences = sent_tokenize(article_body)\r\n        parsed_sentences = []\r\n\r\n        for sentence in sentences:\r\n            discard_sentence = False\r\n            spin_groups = gen_phrases(sentence)\r\n\r\n            if is_nested(sentence):\r\n                continue\r\n\r\n            parsed_spin_groups = []\n            for spin_group in spin_groups:\r\n                parsed_spin_group = []\r\n\r\n                for phrase in spin_group:\r\n                    if self.stdize_article:\r\n                        words = (self.stdize_word(x.strip()) for x in phrase.split())\r\n                    else:\r\n                        words = (x.strip() for x in phrase.split())\r\n\r\n                    words_tuple = tuple(word for word in words if (not self.omit_stopwords or word not in STOP_WORDS))\r\n\r\n                    if self.max_phrase_size and len(words_tuple) > self.max_phrase_size:\r\n                        # Just discard the whole sentence\r\n                        discard_sentence = True\r\n                    #Words tuple may be empty if it's just stopwords\r\n                    if words_tuple:\r\n                        parsed_spin_group.append(words_tuple)\r\n\r\n                # Parsed spin group may be empty if we're omitting stopwords. Only add it if it is nonempty\r\n                if parsed_spin_group:\r\n                    parsed_spin_groups.append(frozenset(parsed_spin_group))\r\n            if not discard_sentence:\r\n                parsed_sentences.append(parsed_spin_groups)\r\n\r\n        if store_in_cache:\r\n            self.articles[num]['article_sentences'] = parsed_sentences\r\n\r\n        return parsed_sentences\r\n\r\n    def get_similar_articles(self, num):\r\n        '''\r\n        Returns a list of articles with at least 1\r\n        keyword in common to article num\r\n        '''\r\n        kws = self.get_keywords(num)\r\n        if not kws:\r\n            return set()\r\n        similar_articles = set()\r\n        for kw in kws:\r\n            similar_articles.update(self.keywords.get(kw, set()))\r\n        if num in similar_articles:\r\n            similar_articles.remove(num)\r\n        return similar_articles\r\n\r\n    def get_very_similar_articles(self, num):\r\n        '''\r\n        Returns a list of articles with all\r\n        keywords in common to article num\r\n        '''\r\n        kws = self.get_keywords(num)\r\n        if not kws:\r\n            return set()\r\n        similar_articles = self.keywords.get(kws.pop(), set())\r\n        for kw in kws:\r\n            similar_articles.intersection_update(self.keywords.get(kw, set()))\r\n        if num in similar_articles:\r\n            similar_articles.remove(num)\r\n        return similar_articles\r\n\r\n    def get_phrase_size_stats(self):\r\n        max_phrase_size, max_group = 0, None\r\n        counts = {}\r\n        total = 0.0\r\n        seen_phrases = set()\r\n        for num in xrange(self.count):\r\n            for sentence in self.get_article_sentences(num):\r\n                for spin_group in sentence:\r\n                    for phrase in spin_group:\r\n                        phrase_size = len(phrase)\r\n                        if phrase_size == 0 or phrase in seen_phrases:\r\n                            continue\r\n                        seen_phrases.add(phrase)\r\n                        counts[phrase_size] = counts.get(phrase_size, 0) + 1\r\n                        total += 1\r\n                        if phrase_size > max_phrase_size:\r\n                            max_phrase_size, max_phrase = phrase_size, phrase\r\n\r\n        print \"Max phrase size : \", max_phrase_size\r\n        print max_phrase\r\n        print \"\"\r\n        for k, v in counts.iteritems():\r\n            print \"{0} : {1}\".format(k, v)\r\n\r\n        for k, v in counts.iteritems():\r\n            print \"{0} : {1}\".format(k, v / total)\r\n\r\n        cumulative = 0.0\r\n\r\n        for k, v in counts.iteritems():\r\n            cumulative += v / total\r\n            print \"{0} : {1}\".format(k, cumulative)\r\n\r\n\r\n\r\ndef cosine(a, b):\r\n    def dot(a, b):\r\n        dot = 0\r\n        for k, v in a.iteritems():\r\n            if k in b:\r\n                dot += a[k] + b[k]\r\n        return float(dot)\r\n    mag = lambda x : math.sqrt(dot(x, x))\r\n    sim = lambda x, y : dot(x, y) / (mag(x) * mag(y))\r\n\r\n    aa = collections.Counter(a.split())\r\n    bb = collections.Counter(b.split())\r\n\r\n    return sim(aa, bb)\r\n\r\n\r\ndef gen_phrases(s):\r\n    '''\r\n    Makes a set of phrases\r\n    '''\r\n    # TODO: make sure exclude set is OK\r\n    exclude = set(string.punctuation) - set([' ', '|', '{', '}', '\\''])\r\n    s = ''.join(ch for ch in s.lower() if ch not in exclude)\r\n    crude_split = re.split(\"\\{(.+?)\\}|(\\w+)\", s)\r\n\r\n    return [ \r\n        k for k in \r\n            [ [w for w in x.split(\"|\") if w] for x in crude_split if (x and x.strip()) ] \r\n        if k \r\n    ]\r\n\r\n\r\ndef is_nested(text):\r\n    '''\r\n    Returns true if text has a spin group within a spin group\r\n    '''\r\n    cnt = 0\r\n    for ch in text:\r\n        if ch == \"{\":\r\n            cnt += 1\r\n        if ch == \"}\":\r\n            cnt -= 1\r\n        if cnt > 1:\r\n            return True\r\n    return False\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    articles = SourceArticles(            \r\n            #stdizer=PORTER_STEMMER,\r\n            omit_stopwords=True,\r\n            stdize_article=True,\r\n            max_phrase_size=None\r\n    )\r\n\r\n    '''\r\n    print articles.get_article_sentences(1, article_body=\"I {like|love} the {dog|canine}. {He doesn't care for|He really does not care for|He really doesn't care for} the dog. She {like|love} the dumb dog\")\r\n\r\n    # Print all keywords\r\n    for i in range(articles.count):\r\n        print \"ARTICLE {0}\".format(i)\r\n        for kw in articles.get_keywords(i):\r\n            print \"\\t\" + kw\r\n    print \"\\n\\n\"\r\n    # Print all keyword associations\r\n    for keyword, nums in articles.keywords.iteritems():\r\n        if len(nums) > 1:\r\n            print \"{0}\\n\\tARTICLES: {1}\".format(keyword, nums)\r\n\r\n    print \"----------------------------------------------------\"\r\n    print \"{0} TOTAL ARTICLES\\n\".format(articles.count)\r\n\r\n    print articles.get_similar_articles(313)\r\n\r\n    print \"----------------------------------------------------\"\r\n    '''\r\n    \r\n    article_num = 0\r\n\r\n    print \"SOURCE ARTICLE:\\n{0}\\n\".format(articles.get_article(article_num))\r\n\r\n    a = articles.spin_dissimilar_articles(article_num, 2)\r\n    a1 = \" \".join(a[0])\r\n    a2 = \" \".join(a[1])\r\n\r\n    print \"ARTICLE 1:\\n{0}\\n\".format(a1)\r\n    print \"ARTICLE 2:\\n{0}\\n\".format(a2)\r\n\r\n    print \"Cosine Similarity: {0}\".format(cosine(a1, a2))\r\n\r\n    print \"\\n\\n\\nTesting Get Article Sentences\\n\\n\"\r\n\r\n\r\n"]],"start1":0,"start2":0,"length1":0,"length2":14201}]],"length":14201}
{"contributors":["wjk56@cornell.edu","jeremy.d.fein@gmail.com"],"silentsave":true,"ts":1354038207143,"patch":[[{"diffs":[[0,"\nimport "],[-1,"collections"],[0,"\r\nimport"]],"start1":122,"start2":122,"length1":27,"length2":16},{"diffs":[[0,"ups = []"],[1,"\r"],[0,"\n       "]],"start1":7964,"start2":7964,"length1":16,"length2":17},{"diffs":[[0,"b.split())\r\n"],[1,"    '''\r\n    count_a = {}\r\n    count_b = {}\r\n    \r\n    for term in a.split():\r\n    '''"],[0,"\r\n    return"]],"start1":11910,"start2":11910,"length1":24,"length2":110}]],"length":14277,"saved":false}
{"ts":1354038211820,"patch":[[{"diffs":[[0,"\n'''\r\n\r\n"],[-1,"import "],[1,"from counter import Counter"],[0,"\r\nimport"]],"start1":115,"start2":115,"length1":23,"length2":43}]],"length":14297,"saved":false}
{"ts":1354038253631,"patch":[[{"diffs":[[0,"   aa = "],[-1,"collections."],[0,"Counter("]],"start1":11861,"start2":11861,"length1":28,"length2":16}]],"length":14285,"saved":false}
{"ts":1354038256127,"patch":[[{"diffs":[[0,"b = "],[-1,"collections."],[0,"Coun"]],"start1":11894,"start2":11894,"length1":20,"length2":8}]],"length":14273,"saved":false}
{"ts":1354038259196,"patch":[[{"diffs":[[0,"))\r\n"],[-1,"    '''\r\n    count_a = {}\r\n    count_b = {}\r\n    \r\n    for term in a.split():\r\n    '''"],[0,"\r\n  "]],"start1":11914,"start2":11914,"length1":94,"length2":8}]],"length":14187,"saved":false}
{"contributors":[],"silentsave":false,"ts":1354038394620,"patch":[[{"diffs":[[0,"\r\n\r\n"],[-1,"from counter import Counter"],[1,"import collections"],[0,"\r\nim"]],"start1":119,"start2":119,"length1":35,"length2":26}]],"length":14178,"saved":false}
{"ts":1354038404205,"patch":[[{"diffs":[[0,"\r\n\r\n"],[-1,"import collections"],[1,"from counter import Counter"],[0,"\r\nim"]],"start1":119,"start2":119,"length1":26,"length2":35}]],"length":14187,"saved":false}
